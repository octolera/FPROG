# Ввод-вывод и работа с данными
Apache Spark - платформа распределенных вычислений, позволяющая параллельно обрабатывать большие объёмы данных в кластере компьютеров. Представляет унифицированный механизм для обработки распределенных данных со встроенными модулями для БД, машинного обучения, обработки графов
Первоначально разработан с использованием scala, API-интерфейсы доступны на python, scala, java, R.
методы 

sc.parallelize(Seq(1,2,3,4,5)) - созд распределенную коллекцию данных.

## Кластер spark 
Распределенная вычислительная среда из нескольких узлов, объединенная в единую систему
### Роли в кластере
+ master node
+ worker node

## RDD(resilient distributed dataset)
Неизменяемый распределенный набор объектов. Каждый набор в RDD разделен на логические разделы, вычисляемые на разных узлов кластера.

var list Rdd = sc.parralelize(List(1,2,3,4,5))
val flatFile = listRdd.map(x=> x + 2)

# Функции
### count()
Возвр число элементов в наборе Rdd. Каждый узел кластера подсчитывает количество эл-тов в своей локальной части данных, результат суммируется
### countApprox()
Приближенное число элементов в наборе если достигнут таймаут, но счёт элементов не закончен. Используется в этих ваших бигдатах.
### countApproxDistinct()
Приближенное число различных элементов в наборе. Интервальная оценка.
### countByValue() 
Ипользуется для подсчета количества вхождений каждого уникального значения в RDD. Результат в виде пары ключ-значение.
### countByValueApprox()
Используется для приближенного подсчета каждого уникального значения в RDD. Второй параметр - коэф. доверия (0-1). Используется в этих ваших бигдатах, шоп время экономить, а то предыдущий вах дорого.
Возвращает мап с интервалами в доверительных границах.
### take()
возвращает первые n элементов. top(), для маленьких наборов.
### takeOrdered() 
первые num наименьших/наибольших
### takeSample()
случайная выборка из n эл-тов




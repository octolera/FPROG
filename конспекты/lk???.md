# Ввод-вывод и работа с данными
Apache Spark - платформа распределенных вычислений, позволяющая параллельно обрабатывать большие объёмы данных в кластере компьютеров. Представляет унифицированный механизм для обработки распределенных данных со встроенными модулями для БД, машинного обучения, обработки графов
Первоначально разработан с использованием scala, API-интерфейсы доступны на python, scala, java, R.
методы 

sc.parallelize(Seq(1,2,3,4,5)) - созд распределенную коллекцию данных.

## Кластер spark 
Распределенная вычислительная среда из нескольких узлов, объединенная в единую систему
### Роли в кластере
+ master node
+ worker node

## RDD(resilient distributed dataset)
Неизменяемый распределенный набор объектов. Каждый набор в RDD разделен на логические разделы, вычисляемые на разных узлов кластера.

## 
var list Rdd = sc.parralelize(List(1,2,3,4,5))
val flatFile = listRdd.map(x=> x + 2)

# Функции
### count()
Возвр число элементов в наборе Rdd. Каждый узел кластера подсчитывает количество эл-тов в своей локальной части данных, результат суммируется
### countApprox()
Приближенное число элементов в наборе если достигнут таймаут, но счёт элементов не закончен. Используется в этих ваших бигдатах.
### countApproxDistinct()
Приближенное число различных элементов в наборе.
